{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNL//U1DXp7NaJ6PVss2K0g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DL4CV-NPTEL/DL4CV_NPTEL_2024_notebooks/blob/main/DL4CV_Week7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recurrent Neural Network"
      ],
      "metadata": {
        "id": "_oeyumQjUS1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "1AEVLnPtLJSc"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = ['hey how are you','good i am fine','have a nice day']\n",
        "\n",
        "# Join all the sentences together and extract the unique characters from the combined sentences\n",
        "chars = set(''.join(text))\n",
        "\n",
        "# Creating a dictionary that maps integers to the characters\n",
        "int2char = dict(enumerate(chars))\n",
        "\n",
        "# Creating another dictionary that maps characters to integers\n",
        "char2int = {char: ind for ind, char in int2char.items()}"
      ],
      "metadata": {
        "id": "Osy0q1FLLJuM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(char2int)"
      ],
      "metadata": {
        "id": "WNk4ABtyLLz6",
        "outputId": "56d7d753-3b99-4336-fdb6-8b116ebff6f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'i': 0, 'a': 1, 'h': 2, 'w': 3, 'm': 4, 'c': 5, 'f': 6, 'r': 7, 'g': 8, 'e': 9, 'u': 10, 'v': 11, 'd': 12, 'n': 13, ' ': 14, 'y': 15, 'o': 16}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "maxlen = len(max(text, key=len))\n",
        "print(\"The longest string has {} characters\".format(maxlen))"
      ],
      "metadata": {
        "id": "Kb96Hl8MLNFa",
        "outputId": "90b80548-f051-4dbf-b495-27669d9a95e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The longest string has 15 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Padding\n",
        "\n",
        "# A simple loop that loops through the list of sentences and adds a ' ' whitespace until the length of the sentence matches\n",
        "# the length of the longest sentence\n",
        "for i in range(len(text)):\n",
        "    while len(text[i])<maxlen:\n",
        "        text[i] += ' '"
      ],
      "metadata": {
        "id": "fEy3GBykLO57"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating lists that will hold our input and target sequences\n",
        "input_seq = []\n",
        "target_seq = []\n",
        "\n",
        "for i in range(len(text)):\n",
        "    # Remove last character for input sequence\n",
        "    input_seq.append(text[i][:-1])\n",
        "\n",
        "    # Remove firsts character for target sequence\n",
        "    target_seq.append(text[i][1:])\n",
        "    print(\"Input Sequence: {}\\nTarget Sequence: {}\".format(input_seq[i], target_seq[i]))"
      ],
      "metadata": {
        "id": "Fn1E16jELQi6",
        "outputId": "ad1bc3c6-e52e-47b4-ef50-920e4cd93505",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Sequence: hey how are yo\n",
            "Target Sequence: ey how are you\n",
            "Input Sequence: good i am fine\n",
            "Target Sequence: ood i am fine \n",
            "Input Sequence: have a nice da\n",
            "Target Sequence: ave a nice day\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(text)):\n",
        "    input_seq[i] = [char2int[character] for character in input_seq[i]]\n",
        "    target_seq[i] = [char2int[character] for character in target_seq[i]]"
      ],
      "metadata": {
        "id": "SpoNVqhnLSEq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_size = len(char2int)\n",
        "seq_len = maxlen - 1\n",
        "batch_size = len(text)\n",
        "\n",
        "def one_hot_encode(sequence, dict_size, seq_len, batch_size):\n",
        "    # Creating a multi-dimensional array of zeros with the desired output shape\n",
        "    features = np.zeros((batch_size, seq_len, dict_size), dtype=np.float32)\n",
        "\n",
        "    # Replacing the 0 at the relevant character index with a 1 to represent that character\n",
        "    for i in range(batch_size):\n",
        "        for u in range(seq_len):\n",
        "            features[i, u, sequence[i][u]] = 1\n",
        "    return features"
      ],
      "metadata": {
        "id": "jay0WkkqLTkZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_seq = one_hot_encode(input_seq, dict_size, seq_len, batch_size)\n",
        "print(\"Input shape: {} --> (Batch Size, Sequence Length, One-Hot Encoding Size)\".format(input_seq.shape))"
      ],
      "metadata": {
        "id": "m3QvPx_gLVkJ",
        "outputId": "22b511a9-d4e6-436c-ce5e-1804dbe1d000",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: (3, 14, 17) --> (Batch Size, Sequence Length, One-Hot Encoding Size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_seq = torch.from_numpy(input_seq)\n",
        "target_seq = torch.Tensor(target_seq)"
      ],
      "metadata": {
        "id": "z2JefTWCLXU6"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
        "is_cuda = torch.cuda.is_available()\n",
        "\n",
        "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU not available, CPU used\")"
      ],
      "metadata": {
        "id": "8w3fgPycLYiB",
        "outputId": "78e5965e-e982-497b-9c39-93dbca741fae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is available\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        # Defining some parameters\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        #Defining the layers\n",
        "        # RNN Layer\n",
        "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        #Initializing hidden state for first input using method defined below\n",
        "        hidden = self.init_hidden(batch_size)\n",
        "\n",
        "        # Passing in the input and hidden state into the model and obtaining outputs\n",
        "        out, hidden = self.rnn(x, hidden)\n",
        "\n",
        "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
        "        out = out.contiguous().view(-1, self.hidden_dim)\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
        "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
        "         # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
        "        return hidden"
      ],
      "metadata": {
        "id": "OyGbLs4vLZ-K"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model with hyperparameters\n",
        "model = Model(input_size=dict_size, output_size=dict_size, hidden_dim=12, n_layers=1)\n",
        "# We'll also set the model to the device that we defined earlier (default is CPU)\n",
        "model = model.to(device)\n",
        "\n",
        "# Define hyperparameters\n",
        "n_epochs = 100\n",
        "lr=0.01\n",
        "\n",
        "# Define Loss, Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "uXiMvGYvLdCi"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Run\n",
        "input_seq = input_seq.to(device)\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
        "    #input_seq = input_seq.to(device)\n",
        "    output, hidden = model(input_seq)\n",
        "    output = output.to(device)\n",
        "    target_seq = target_seq.to(device)\n",
        "    loss = criterion(output, target_seq.view(-1).long())\n",
        "    loss.backward() # Does backpropagation and calculates gradients\n",
        "    optimizer.step() # Updates the weights accordingly\n",
        "\n",
        "    if epoch%10 == 0:\n",
        "        print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
        "        print(\"Loss: {:.4f}\".format(loss.item()))"
      ],
      "metadata": {
        "id": "YS_XQT4-Levp",
        "outputId": "0a984432-539a-45d2-bf6b-490a3993fb92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10/100............. Loss: 2.4298\n",
            "Epoch: 20/100............. Loss: 2.0362\n",
            "Epoch: 30/100............. Loss: 1.6017\n",
            "Epoch: 40/100............. Loss: 1.1747\n",
            "Epoch: 50/100............. Loss: 0.8103\n",
            "Epoch: 60/100............. Loss: 0.5370\n",
            "Epoch: 70/100............. Loss: 0.3549\n",
            "Epoch: 80/100............. Loss: 0.2407\n",
            "Epoch: 90/100............. Loss: 0.1737\n",
            "Epoch: 100/100............. Loss: 0.1342\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, character):\n",
        "    # One-hot encoding our input to fit into the model\n",
        "    character = np.array([[char2int[c] for c in character]])\n",
        "    character = one_hot_encode(character, dict_size, character.shape[1], 1)\n",
        "    character = torch.from_numpy(character)\n",
        "    character = character.to(device)\n",
        "\n",
        "    out, hidden = model(character)\n",
        "\n",
        "    prob = nn.functional.softmax(out[-1], dim=0).data\n",
        "    # Taking the class with the highest probability score from the output\n",
        "    char_ind = torch.max(prob, dim=0)[1].item()\n",
        "\n",
        "    return int2char[char_ind], hidden"
      ],
      "metadata": {
        "id": "XYu3DitBLkvd"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(model, out_len, start='hey'):\n",
        "    model.eval() # eval mode\n",
        "    start = start.lower()\n",
        "    # First off, run through the starting characters\n",
        "    chars = [ch for ch in start]\n",
        "    size = out_len - len(chars)\n",
        "    # Now pass in the previous characters and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(model, chars)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ],
      "metadata": {
        "id": "mtWxIt0MLmY6"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample(model, 15, 'good')"
      ],
      "metadata": {
        "id": "TyJ9AK4NLn5s",
        "outputId": "ab6d597c-cf41-4fba-8ec9-07aff7669f29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'good i am fine '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "7S2sujJWMBtK"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTMs and GRUs\n",
        "\n"
      ],
      "metadata": {
        "id": "5p8aGbdxc1Nz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "from tqdm import tqdm_notebook\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Define data root directory\n",
        "data_dir = \"https://raw.githubusercontent.com/gabrielloye/GRU_Prediction/master/data/\"\n",
        "\n",
        "time.clock = time.time"
      ],
      "metadata": {
        "id": "MtImX4Nq8z8B"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualise how our data looks\n",
        "pd.read_csv(data_dir + 'AEP_hourly.csv').head()"
      ],
      "metadata": {
        "id": "-QA5nbDr80gM",
        "outputId": "2728598c-fc81-413d-b553-43eedea916b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              Datetime   AEP_MW\n",
              "0  2004-12-31 01:00:00  13478.0\n",
              "1  2004-12-31 02:00:00  12865.0\n",
              "2  2004-12-31 03:00:00  12577.0\n",
              "3  2004-12-31 04:00:00  12517.0\n",
              "4  2004-12-31 05:00:00  12670.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f0e9bc63-4579-46d6-9e5f-d0651ff7b5ea\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Datetime</th>\n",
              "      <th>AEP_MW</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2004-12-31 01:00:00</td>\n",
              "      <td>13478.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2004-12-31 02:00:00</td>\n",
              "      <td>12865.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2004-12-31 03:00:00</td>\n",
              "      <td>12577.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2004-12-31 04:00:00</td>\n",
              "      <td>12517.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2004-12-31 05:00:00</td>\n",
              "      <td>12670.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f0e9bc63-4579-46d6-9e5f-d0651ff7b5ea')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f0e9bc63-4579-46d6-9e5f-d0651ff7b5ea button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f0e9bc63-4579-46d6-9e5f-d0651ff7b5ea');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e83dc3ab-aebb-4e22-baea-693a7f94885c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e83dc3ab-aebb-4e22-baea-693a7f94885c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e83dc3ab-aebb-4e22-baea-693a7f94885c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Datetime\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"2004-12-31 02:00:00\",\n          \"2004-12-31 05:00:00\",\n          \"2004-12-31 03:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"AEP_MW\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 389.97987127542876,\n        \"min\": 12517.0,\n        \"max\": 13478.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          12865.0,\n          12670.0,\n          12577.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The scaler objects will be stored in this dictionary so that our output test data from the model can be re-scaled during evaluation\n",
        "label_scalers = {}\n",
        "\n",
        "train_x = []\n",
        "test_x = {}\n",
        "test_y = {}\n",
        "\n",
        "dir_list = ['COMED_hourly.csv','DAYTON_hourly.csv','DEOK_hourly.csv','DOM_hourly.csv','DUQ_hourly.csv',\n",
        "            'EKPC_hourly.csv','FE_hourly.csv','NI_hourly.csv','PJME_hourly.csv','PJMW_hourly.csv',\n",
        "            'PJM_Load_hourly.csv','pjm_hourly_est.csv']\n",
        "for file in dir_list:\n",
        "    # Skipping the files we're not using\n",
        "    if file[-4:] != \".csv\" or file == \"pjm_hourly_est.csv\":\n",
        "        continue\n",
        "\n",
        "    # Store csv file in a Pandas DataFrame\n",
        "    df = pd.read_csv('{}/{}'.format(data_dir, file), parse_dates=[0])\n",
        "    # Processing the time data into suitable input formats\n",
        "    df['hour'] = df.apply(lambda x: x['Datetime'].hour,axis=1)\n",
        "    df['dayofweek'] = df.apply(lambda x: x['Datetime'].dayofweek,axis=1)\n",
        "    df['month'] = df.apply(lambda x: x['Datetime'].month,axis=1)\n",
        "    df['dayofyear'] = df.apply(lambda x: x['Datetime'].dayofyear,axis=1)\n",
        "    df = df.sort_values(\"Datetime\").drop(\"Datetime\",axis=1)\n",
        "\n",
        "    # Scaling the input data\n",
        "    sc = MinMaxScaler()\n",
        "    label_sc = MinMaxScaler()\n",
        "    data = sc.fit_transform(df.values)\n",
        "    # Obtaining the Scale for the labels(usage data) so that output can be re-scaled to actual value during evaluation\n",
        "    label_sc.fit(df.iloc[:,0].values.reshape(-1,1))\n",
        "    label_scalers[file] = label_sc\n",
        "\n",
        "    # Define lookback period and split inputs/labels\n",
        "    lookback = 90\n",
        "    inputs = np.zeros((len(data)-lookback,lookback,df.shape[1]))\n",
        "    labels = np.zeros(len(data)-lookback)\n",
        "\n",
        "    for i in range(lookback, len(data)):\n",
        "        inputs[i-lookback] = data[i-lookback:i]\n",
        "        labels[i-lookback] = data[i,0]\n",
        "    inputs = inputs.reshape(-1,lookback,df.shape[1])\n",
        "    labels = labels.reshape(-1,1)\n",
        "\n",
        "    # Split data into train/test portions and combining all data from different files into a single array\n",
        "    test_portion = int(0.1*len(inputs))\n",
        "    if len(train_x) == 0:\n",
        "        train_x = inputs[:-test_portion]\n",
        "        train_y = labels[:-test_portion]\n",
        "    else:\n",
        "        train_x = np.concatenate((train_x,inputs[:-test_portion]))\n",
        "        train_y = np.concatenate((train_y,labels[:-test_portion]))\n",
        "    test_x[file] = (inputs[-test_portion:])\n",
        "    test_y[file] = (labels[-test_portion:])\n"
      ],
      "metadata": {
        "id": "81NUwu8P8486"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1024\n",
        "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)"
      ],
      "metadata": {
        "id": "ue011xcC88VJ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
        "is_cuda = torch.cuda.is_available()\n",
        "\n",
        "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "9-n1YCkbEVA7"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GRUNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):\n",
        "        super(GRUNet, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.gru = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x, h):\n",
        "        out, h = self.gru(x, h)\n",
        "        out = self.fc(self.relu(out[:,-1]))\n",
        "        return out, h\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
        "        return hidden\n",
        "\n",
        "class LSTMNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):\n",
        "        super(LSTMNet, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x, h):\n",
        "        out, h = self.lstm(x, h)\n",
        "        out = self.fc(self.relu(out[:,-1]))\n",
        "        return out, h\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
        "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
        "        return hidden"
      ],
      "metadata": {
        "id": "qOCiBhsGEXjE"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_loader, learn_rate, hidden_dim=256, EPOCHS=3, model_type=\"GRU\"):\n",
        "\n",
        "    # Setting common hyperparameters\n",
        "    input_dim = next(iter(train_loader))[0].shape[2]\n",
        "    output_dim = 1\n",
        "    n_layers = 2\n",
        "    # Instantiating the models\n",
        "    if model_type == \"GRU\":\n",
        "        model = GRUNet(input_dim, hidden_dim, output_dim, n_layers)\n",
        "    else:\n",
        "        model = LSTMNet(input_dim, hidden_dim, output_dim, n_layers)\n",
        "    model.to(device)\n",
        "\n",
        "    # Defining loss function and optimizer\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
        "\n",
        "    model.train()\n",
        "    print(\"Starting Training of {} model\".format(model_type))\n",
        "    epoch_times = []\n",
        "    # Start training loop\n",
        "    for epoch in range(1,EPOCHS+1):\n",
        "        start_time = time.clock()\n",
        "        h = model.init_hidden(batch_size)\n",
        "        avg_loss = 0.\n",
        "        counter = 0\n",
        "        for x, label in train_loader:\n",
        "            counter += 1\n",
        "            if model_type == \"GRU\":\n",
        "                h = h.data\n",
        "            else:\n",
        "                h = tuple([e.data for e in h])\n",
        "            model.zero_grad()\n",
        "\n",
        "            out, h = model(x.to(device).float(), h)\n",
        "            loss = criterion(out, label.to(device).float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            avg_loss += loss.item()\n",
        "            if counter%200 == 0:\n",
        "                print(\"Epoch {}......Step: {}/{}....... Average Loss for Epoch: {}\".format(epoch, counter, len(train_loader), avg_loss/counter))\n",
        "        current_time = time.clock()\n",
        "        print(\"Epoch {}/{} Done, Total Loss: {}\".format(epoch, EPOCHS, avg_loss/len(train_loader)))\n",
        "        print(\"Total Time Elapsed: {} seconds\".format(str(current_time-start_time)))\n",
        "        epoch_times.append(current_time-start_time)\n",
        "    print(\"Total Training Time: {} seconds\".format(str(sum(epoch_times))))\n",
        "    return model\n",
        "\n",
        "def evaluate(model, test_x, test_y, label_scalers):\n",
        "    model.eval()\n",
        "    outputs = []\n",
        "    targets = []\n",
        "    start_time = time.clock()\n",
        "    for i in test_x.keys():\n",
        "        inp = torch.from_numpy(np.array(test_x[i]))\n",
        "        labs = torch.from_numpy(np.array(test_y[i]))\n",
        "        h = model.init_hidden(inp.shape[0])\n",
        "        out, h = model(inp.to(device).float(), h)\n",
        "        outputs.append(label_scalers[i].inverse_transform(out.cpu().detach().numpy()).reshape(-1))\n",
        "        targets.append(label_scalers[i].inverse_transform(labs.numpy()).reshape(-1))\n",
        "    print(\"Evaluation Time: {}\".format(str(time.clock()-start_time)))\n",
        "    sMAPE = 0\n",
        "    for i in range(len(outputs)):\n",
        "        sMAPE += np.mean(abs(outputs[i]-targets[i])/(targets[i]+outputs[i])/2)/len(outputs)\n",
        "    print(\"sMAPE: {}%\".format(sMAPE*100))\n",
        "    return outputs, targets, sMAPE"
      ],
      "metadata": {
        "id": "DB8sWpuTEaV8"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.001\n",
        "\n"
      ],
      "metadata": {
        "id": "dvtC2Dd8Ejkz"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Either execute GRU or LSTM, dont execute both in same run, you will run out of memory"
      ],
      "metadata": {
        "id": "pysSwl-KmHgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gru_model = train(train_loader, lr, model_type=\"GRU\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsQrqFGJl9yJ",
        "outputId": "e7991431-00a4-4c58-de0e-2b568cfe455f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Training of GRU model\n",
            "Epoch 1......Step: 200/850....... Average Loss for Epoch: 0.006070715197420213\n",
            "Epoch 1......Step: 400/850....... Average Loss for Epoch: 0.003367337575982674\n",
            "Epoch 1......Step: 600/850....... Average Loss for Epoch: 0.0023812185681405633\n",
            "Epoch 1......Step: 800/850....... Average Loss for Epoch: 0.0018650112095019722\n",
            "Epoch 1/3 Done, Total Loss: 0.0017708871407804134\n",
            "Total Time Elapsed: 125.02558398246765 seconds\n",
            "Epoch 2......Step: 200/850....... Average Loss for Epoch: 0.00025264622083341235\n",
            "Epoch 2......Step: 400/850....... Average Loss for Epoch: 0.00023870613433246034\n",
            "Epoch 2......Step: 600/850....... Average Loss for Epoch: 0.00022882276326223897\n",
            "Epoch 2......Step: 800/850....... Average Loss for Epoch: 0.00022073312451539095\n",
            "Epoch 2/3 Done, Total Loss: 0.00021807167507872422\n",
            "Total Time Elapsed: 128.94692397117615 seconds\n",
            "Epoch 3......Step: 200/850....... Average Loss for Epoch: 0.0001771311951597454\n",
            "Epoch 3......Step: 400/850....... Average Loss for Epoch: 0.00017197611250594492\n",
            "Epoch 3......Step: 600/850....... Average Loss for Epoch: 0.00017009032304486028\n",
            "Epoch 3......Step: 800/850....... Average Loss for Epoch: 0.00016717579884243606\n",
            "Epoch 3/3 Done, Total Loss: 0.00016662941676365923\n",
            "Total Time Elapsed: 130.25070309638977 seconds\n",
            "Total Training Time: 384.22321105003357 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gru_outputs, targets, gru_sMAPE = evaluate(gru_model, test_x, test_y, label_scalers)"
      ],
      "metadata": {
        "id": "Q8KEJ91QElB_",
        "outputId": "b98170f8-4798-48a4-cbe4-30a0a049da75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Time: 3.657687187194824\n",
            "sMAPE: 0.3056258609800797%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lstm_model = train(train_loader, lr, model_type=\"LSTM\")"
      ],
      "metadata": {
        "id": "iL3vS5lUl-63"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lstm_outputs, targets, lstm_sMAPE = evaluate(Lstm_model, test_x, test_y, label_scalers)"
      ],
      "metadata": {
        "id": "cl-wVFjZEr1q"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4sTHolHPEtet"
      },
      "execution_count": 30,
      "outputs": []
    }
  ]
}